# wrapper.py

import runpy
import spacy
import torch
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

# â€”â€” 1) spaCy GPU bindirmesini dene, baÅŸarÄ±sÄ±zsa devam et
try:
    spacy.require_gpu()
    print("spaCy GPUâ€™da Ã§alÄ±ÅŸÄ±yor âœ…")
except Exception as e:
    print(f"spaCy GPUâ€™a geÃ§emedi ({e}), CPUâ€™da devam ediliyor âš ï¸")

# â€”â€” 2) Model.from_pretrainedâ€™i yakalayarak GPUâ€™ya taÅŸÄ±
_orig_from_pretrained = AutoModelForSeq2SeqLM.from_pretrained
def _gpu_from_pretrained(*args, **kwargs):
    model = _orig_from_pretrained(*args, **kwargs)
    if torch.cuda.is_available():
        model.to("cuda")
        print("Transformer modeli GPUâ€™ya taÅŸÄ±ndÄ± ğŸš€")
    else:
        print("CUDA bulunamadÄ±, model CPUâ€™da kalacak")
    return model
AutoModelForSeq2SeqLM.from_pretrained = _gpu_from_pretrained

# â€”â€” 3) Tokenizer() Ã§Ä±ktÄ±larÄ±ndaki tensorâ€™larÄ± da GPUâ€™ya gÃ¶nder
_orig_tokenize = AutoTokenizer.__call__
def _gpu_tokenize(self, *args, **kwargs):
    toks = _orig_tokenize(self, *args, **kwargs)
    if torch.cuda.is_available():
        for k, v in toks.items():
            if hasattr(v, "to"):
                toks[k] = v.to("cuda")
    return toks
AutoTokenizer.__call__ = _gpu_tokenize

# â€”â€” 4) Orijinal chunk.pyâ€™yi hiÃ§ deÄŸiÅŸtirmeden Ã§alÄ±ÅŸtÄ±r
runpy.run_path("C:\\Users\\user\\OneDrive\\Belgeler\\GitHub\\Lgram\\models\\chunk.py", run_name="__main__")
