#!/usr/bin/env python3
"""
Text Sources Analysis: Where do the pattern learning texts actually come from?
Complete analysis of all text sources used for pattern learning
"""

import sys
import os

# Add project root to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

def analyze_text_sources():
    """Analyze all sources of text used for pattern learning"""
    print("ğŸ” PATTERN Ã–ÄRENMESÄ° Ä°Ã‡Ä°N KULLANILAN METÄ°N KAYNAKLARI")
    print("=" * 70)
    
    print("ğŸ“š MEVCUT METÄ°N KAYNAKLARI:")
    
    # 1. Training data files
    print(f"\n1ï¸âƒ£ TEMEL EÄÄ°TÄ°M VERÄ°SETLERÄ°:")
    
    # text_data.txt
    text_data_path = r"c:\Users\user\OneDrive\Belgeler\GitHub\Lgram\ngrams\text_data.txt"
    if os.path.exists(text_data_path):
        size = os.path.getsize(text_data_path) / (1024 * 1024)  # MB
        with open(text_data_path, 'r', encoding='utf-8', errors='ignore') as f:
            lines = sum(1 for _ in f)
        print(f"   ğŸ“– text_data.txt: {size:.2f}MB, ~{lines:,} lines")
        print(f"      Ä°Ã§erik: Klasik edebiyat metinleri (Edgar Allan Poe)")
        print(f"      AmaÃ§: N-gram modeli eÄŸitimi")
        print(f"      Kaynak: Project Gutenberg klasikleri")
        
        # Show sample
        with open(text_data_path, 'r', encoding='utf-8', errors='ignore') as f:
            sample = f.read(200)
        print(f"      Ã–rnek: {sample[:100]}...")
    
    # more.txt
    more_path = r"c:\Users\user\OneDrive\Belgeler\GitHub\Lgram\ngrams\more.txt"
    if os.path.exists(more_path):
        size = os.path.getsize(more_path) / 1024  # KB
        with open(more_path, 'r', encoding='utf-8', errors='ignore') as f:
            lines = sum(1 for _ in f)
        print(f"\n   ğŸ“ more.txt: {size:.2f}KB, ~{lines:,} lines")
        print(f"      Ä°Ã§erik: KÄ±sa cÃ¼mle Ã¶rnekleri")
        print(f"      AmaÃ§: Basit pattern Ã¶rnekleri")
        print(f"      Format: Akademik kelime kombinasyonlarÄ±")
        
        # Show sample
        with open(more_path, 'r', encoding='utf-8', errors='ignore') as f:
            sample_lines = [f.readline().strip() for _ in range(3)]
        print(f"      Ã–rnek: {sample_lines}")
    
    # 2. Test files with quality texts
    print(f"\n2ï¸âƒ£ KALÄ°TE TEST METÄ°NLERÄ°:")
    
    test_files = [
        "test_pattern_learning.py",
        "analyze_model_vision.py", 
        "pattern_sources_analysis.py"
    ]
    
    for test_file in test_files:
        file_path = os.path.join(r"c:\Users\user\OneDrive\Belgeler\GitHub\Lgram", test_file)
        if os.path.exists(file_path):
            print(f"\n   ğŸ“‹ {test_file}:")
            
            # Extract quality texts from test files
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # Find quality text examples
                import re
                quality_patterns = [
                    r'"""(.*?)"""',  # Triple quoted strings
                    r'"([^"]{50,})"'  # Long strings in quotes
                ]
                
                examples = []
                for pattern in quality_patterns:
                    matches = re.findall(pattern, content, re.DOTALL)
                    for match in matches:
                        clean_match = match.strip()
                        if len(clean_match) > 50 and any(word in clean_match.lower() 
                                                       for word in ['artificial', 'intelligence', 'climate', 'brain', 'machine']):
                            examples.append(clean_match[:100] + "...")
                
                if examples:
                    print(f"      Kaliteli Ã¶rnekler bulundu: {len(examples)}")
                    for i, example in enumerate(examples[:2]):
                        print(f"         {i+1}. {example}")
                else:
                    print(f"      Kaliteli metin Ã¶rneÄŸi bulunamadÄ±")
                    
            except Exception as e:
                print(f"      Dosya okunamadÄ±: {e}")
    
    # 3. Daily logs (if used for training)
    print(f"\n3ï¸âƒ£ GÃœNLÄ°K LOG DOSYALARI:")
    
    logs_dir = r"c:\Users\user\OneDrive\Belgeler\GitHub\Lgram\logs"
    if os.path.exists(logs_dir):
        log_files = [f for f in os.listdir(logs_dir) if f.startswith("daily_log_") and f.endswith(".txt")]
        total_size = 0
        
        for log_file in log_files[:5]:  # First 5 files
            log_path = os.path.join(logs_dir, log_file)
            size = os.path.getsize(log_path)
            total_size += size
        
        print(f"   ğŸ“Š Toplam {len(log_files)} log dosyasÄ±")
        print(f"   ğŸ“ Ortalama boyut: {total_size/len(log_files)/1024:.2f}KB")
        print(f"   ğŸ¯ AmaÃ§: Model performans loglarÄ± (pattern Ã¶ÄŸrenmesi iÃ§in deÄŸil)")
        
        # Sample log
        if log_files:
            sample_log = os.path.join(logs_dir, log_files[0])
            try:
                with open(sample_log, 'r', encoding='utf-8', errors='ignore') as f:
                    sample = f.read(100)
                print(f"   ğŸ“ Ã–rnek log: {sample[:50]}...")
            except:
                print(f"   ğŸ“ Log Ã¶rneÄŸi okunamadÄ±")

def analyze_pattern_learning_flow():
    """Show how texts flow into pattern learning"""
    print(f"\nğŸ”„ PATTERN Ã–ÄRENMESÄ° AKIÅI")
    print("=" * 70)
    
    print("ğŸ“¥ METÄ°N GÄ°RÄ°Å KAYNAKLARI:")
    print("   1. Manuel kaliteli metinler (test scriptlerinde)")
    print("   2. KullanÄ±cÄ± tarafÄ±ndan saÄŸlanan referans metinler")
    print("   3. Dinamik olarak analiz edilen metinler")
    print("   4. Code iÃ§inde hardcoded kaliteli Ã¶rnekler")
    
    print(f"\nâš™ï¸  Ä°ÅLEME SÃœRECÄ°:")
    print("   1. Metin learn_from_quality_text() fonksiyonuna girer")
    print("   2. TransitionPatternLearner.learn_from_text() Ã§alÄ±ÅŸÄ±r")
    print("   3. CÃ¼mleler ayrÄ±ÅŸtÄ±rÄ±lÄ±r ve analiz edilir")
    print("   4. Centering Theory analizi yapÄ±lÄ±r")
    print("   5. Transition sequence'ler Ã§Ä±karÄ±lÄ±r")
    print("   6. Pattern'ler frekans ve kalite ile saklanÄ±r")
    
    print(f"\nğŸ’¾ SAKLAMA YERLERÄ°:")
    print("   â€¢ transition_patterns.json â†’ Ana pattern deposu")
    print("   â€¢ SmartCache â†’ HÄ±zlÄ± eriÅŸim cache'i")
    print("   â€¢ Memory â†’ Ã‡alÄ±ÅŸma zamanÄ± geÃ§ici saklama")
    
    print(f"\nğŸ¯ KULLANIM YERLERÄ°:")
    print("   â€¢ generate_coherent_text() â†’ Pattern-based generation")
    print("   â€¢ analyze_text_coherence() â†’ Quality assessment")
    print("   â€¢ generate_coherent_transition_sequence() â†’ Sequence planning")

def show_current_pattern_sources():
    """Show what's actually being used right now"""
    print(f"\nğŸ” ÅU AN AKTÄ°F OLAN PATTERN KAYNAKLARI")
    print("=" * 70)
    
    # Check transition patterns file
    pattern_file = r"c:\Users\user\OneDrive\Belgeler\GitHub\Lgram\lgram\ngrams\transition_patterns.json"
    if os.path.exists(pattern_file):
        import json
        try:
            with open(pattern_file, 'r') as f:
                data = json.load(f)
            
            patterns = data.get('patterns', {})
            
            print(f"âœ… MEVCUT PATTERN'LER: {len(patterns)} adet")
            
            # Analyze pattern sources by examining centers
            all_centers = []
            for pattern_data in patterns.values():
                centers = pattern_data.get('context_centers', [])
                all_centers.extend(centers)
            
            # Count center frequencies
            from collections import Counter
            center_counts = Counter(all_centers)
            
            print(f"\nğŸ“Š EN YAYIN SÃ–YLEM MERKEZLERÄ°:")
            for center, count in center_counts.most_common(10):
                print(f"   {center}: {count} kez kullanÄ±ldÄ±")
            
            print(f"\nğŸ­ PATTERN TÃœRLERÄ°:")
            for pattern_id, pattern_data in list(patterns.items())[:3]:
                sequence = pattern_data['sequence']
                frequency = pattern_data['frequency']
                print(f"   {' â†’ '.join(sequence)} (SÄ±klÄ±k: {frequency})")
                
            # Infer sources from centers
            print(f"\nğŸ” TAHMÄ°NÄ° KAYNAK ANALÄ°ZÄ°:")
            tech_words = ['intelligence', 'systems', 'algorithms', 'data', 'technology']
            science_words = ['research', 'scientists', 'discovery', 'analysis']
            academic_words = ['study', 'knowledge', 'understanding', 'theory']
            
            tech_count = sum(center_counts[word] for word in tech_words if word in center_counts)
            science_count = sum(center_counts[word] for word in science_words if word in center_counts)
            academic_count = sum(center_counts[word] for word in academic_words if word in center_counts)
            
            print(f"   Teknoloji metinleri: ~{tech_count} pattern")
            print(f"   Bilimsel metinler: ~{science_count} pattern")  
            print(f"   Akademik metinler: ~{academic_count} pattern")
            
        except Exception as e:
            print(f"âŒ Pattern dosyasÄ± okunamadÄ±: {e}")
    else:
        print("âŒ Pattern dosyasÄ± bulunamadÄ±")

def main():
    """Main analysis"""
    analyze_text_sources()
    analyze_pattern_learning_flow()
    show_current_pattern_sources()
    
    print(f"\nğŸ† Ã–ZET: METÄ°N KAYNAKLARI")
    print("=" * 70)
    print("âœ… Temel veriset: text_data.txt (Klasik edebiyat)")
    print("âœ… Test Ã¶rnekleri: Hardcoded kaliteli metinler")
    print("âœ… Dinamik input: KullanÄ±cÄ± saÄŸlanan metinler")
    print("âœ… Pattern deposu: transition_patterns.json")
    print("âœ… Aktif learning: Her analiz ile bÃ¼yÃ¼yor")
    
    print(f"\nğŸ¯ SONUÃ‡: Pattern'ler Ã§oÄŸunlukla TEST SCRIPTLER iÃ§indeki")
    print("   hardcoded kaliteli Ã¶rneklerden Ã¶ÄŸreniliyor!")

if __name__ == "__main__":
    main()
